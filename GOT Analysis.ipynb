{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOT Analysis\n",
    "### Import packages\n",
    "This script uses \"codecs\" for reading the text files; \"re\" for regular expressions; \"collections\" for working with tokens; \"nltk\" for natural language toolkit and \"wordcloud\" for creating word clouds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "#from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GOT text files \n",
    "with codecs.open(\"got1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    got1 = f.read()\n",
    "with codecs.open(\"got2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    got2 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword list:\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "#Generate wordcloud data\n",
    "wordcloud1 = WordCloud(stopwords=stopwords, max_words=20, \\\n",
    "                      background_color=\"white\").generate(got1)\n",
    "wordcloud2 = WordCloud(stopwords=stopwords, max_words=20, \\\n",
    "                      background_color=\"white\").generate(got2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display wordclouds\n",
    "import matplotlib.pyplot as mpLib\n",
    "mpLib.imshow(wordcloud1)\n",
    "mpLib.axis(\"off\")\n",
    "mpLib.show()\n",
    "mpLib.imshow(wordcloud2)\n",
    "mpLib.axis(\"off\")\n",
    "mpLib.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOST COMMON WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some specialized functions from NLTK that are not included by default. \n",
    "# It is possible to download just the \"stopwords\" portion but it may be easier to simply download everything in NLTK. \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for English stop words\n",
    "from nltk.corpus import stopwords\n",
    "esw = stopwords.words('english')\n",
    "esw.append(\"would\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter token using regular expressions\n",
    "word_pattern = re.compile(\"^\\w+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Counter Function\n",
    "def get_text_counter(text):\n",
    "    tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "    tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    tokens = [token for token in tokens if re.match(word_pattern, token) and token not in esw]\n",
    "    return collections.Counter(tokens), len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to calculate the absolute frequency and relative frequency of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(counter, size):\n",
    "    abs_freq = np.array([el[1] for el in counter])\n",
    "    rel_freq = abs_freq / size\n",
    "    index = [el[0] for el in counter]\n",
    "    df = pd.DataFrame(data=np.array([abs_freq, rel_freq]).T, index=index, columns=[\"Absolute frequency\", \"Relative frequency\"])\n",
    "    df.index.name = \"Most common words\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the most common words in GOT1, then display the 20 most common.\n",
    "g1_counter, g1_size = get_text_counter(got1)\n",
    "make_df(g1_counter.most_common(20), g1_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 1000 most common words of GOT 1 to csv\n",
    "je_df = make_df(g1_counter.most_common(1000), g1_size)\n",
    "je_df.to_csv(\"G1_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common words of GOT2 then display the 20 most common\n",
    "g2_counter, g2_size = get_text_counter(got2)\n",
    "make_df(g2_counter.most_common(20), g2_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the 1000 most common words of GOT2 to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_df = make_df(g2_counter.most_common(1000), g2_size)\n",
    "wh_df.to_csv(\"G2_1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common words across the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counter = g2_counter + g1_counter\n",
    "all_df = make_df(g2_counter.most_common(1000), 1)\n",
    "most_common_words = all_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with the word frequency differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "for word in most_common_words:\n",
    "    g1_c = g1_counter.get(word, 0) / g1_size\n",
    "    g2_c = g2_counter.get(word, 0) / g2_size\n",
    "    d = abs(g1_c - g2_c)\n",
    "    df_data.append([g1_c, g2_c, d])\n",
    "dist_df = pd.DataFrame(data=df_data, index=most_common_words,\n",
    "                       columns=[\"GOT1 relative frequency\", \"GOT2 relative frequency\",\n",
    "                                \"Relative frequency difference\"])\n",
    "dist_df.index.name = \"Most common words\"\n",
    "dist_df.sort_values(\"Relative frequency difference\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the most distinctive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full list of distinctive words to a csv entitled got.csv\n",
    "dist_df.to_csv(\"GOT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
